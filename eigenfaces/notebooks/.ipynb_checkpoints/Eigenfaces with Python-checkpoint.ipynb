{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA DESCRIPTION\n",
    "\n",
    "For the development of this project, a group of images hosted in a free online database will be used.\n",
    "This database has four different directories holding the images in different levels of difficulty as follows: faces94, faces95, faces96 and grimace. The last two are more complicated due to the images’ variation on background and scale and the type of facial expressions in them.\n",
    "\n",
    "The whole set has 7900 images belonged to 395 individuals. Different genders and races are shown, people wearing glasses and beards are also taken into account and, regarding the age range, the majority of the data corresponds to first year undergraduate students between 18 and 20 years old, even though some older people are present in the data as well.\n",
    "\n",
    "The main features of each directory containing the images are mentioned below:\n",
    "\n",
    "Faces94: this is a collection of images consisting in a wide range of people’s pictures taken while they spoke in front of the camera. Because of the speech, this set is an introduction to the variation on facial expression.\n",
    "Faces94 has 153 individuals’ images using portrait format, it contains pictures of male, female and malestaff in separate directories. The pictures’ background is plain green. It does not have any individual’s variation on head scale and image lighting, but it does have a few on head turn, tilt and slant, and considerable on facial expression.\n",
    "Additionally, there is no individual hairstyle variation as the images were taken in a single session.\n",
    "\n",
    "Faces95: this is a collection of pictures taken while people took one step forward towards the camera. This movement is used to introduce important variations on head scale among the same individual’s images.\n",
    "Faces95 holds 72 people’s images in portrait format, male and female subjects are shown. The background in these images consists of a red curtain. Due to the subjects’ movement at the shooting time, there is some variations on picture’s backgrounds caused by people’s shadows, and also on head scale and image lighting.\n",
    "Some variation is present on facial expression and there is no individual hairstyle variation neither, as the images in this directory were also taken in a single session.\n",
    "\n",
    "Faces96: this collection of facial images holds subjects’ pictures taken while people took one step forward towards the camera as in Faces95. This movement is also used to introduce important variations on head scale among the same individual’s images.\n",
    "This dataset contains 152 individuals’ images in square format and has male and female subjects’ pictures. The background used this time, consists of glossy posters which makes it more complicated than the ones used in Faces94 and Faces95. It has a large variation on head scale and image lighting. It also presents some minor variation on the position of the face in the image, on head turn, tilt and slant and on facial expression.\n",
    "In this set, there is no hairstyle variation neither, as the images were taken in a single session.\n",
    "\n",
    "Grimace: this directory has a collection of subject images taken while people move their heads and make grimaces which get more extreme near the end of the sequence. The other features are similar to those in Faces95.  \n",
    "The number of participants in this dataset is 18 people. The whole set of images is presented in portrait format and it contains images of female and male subjects. The background used in this pictures is plain and there is a very little variation on head scale and image lighting, a bit more on head turn, tilt and slant and a big variation on facial expression.\n",
    "As in the other directories, there is no hairstyle variation in here, as the images were taken in a single session as well.\n",
    "\n",
    "References\n",
    "\n",
    "University of Essex. (2008, June 20). Description of the Collection of Facial Images [online]. Retrieved from https://cswww.essex.ac.uk/mv/allfaces/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "In order to prepare the image dataset to proceed with the final stages of modeling and valuation processes. It is unavoidable to standardize the images features on the initial dataset (codename - faces94) and the external images use it on the future testing activities under the following conditions:\n",
    "\n",
    "**General Images Characteristics**:\n",
    "\n",
    "* File Format *.jpg\n",
    "* Images on Gray Scale.\n",
    "* Size 180x200 for the images on the dataset (codename- faces94)\n",
    "\n",
    "**Activities**:\n",
    "\n",
    "* Organize the images on a short-listed to prepare a new dataset.\n",
    "* Exclude from the dataset all the images without the *.jpg format.\n",
    "* On the OpenCV library of Python, upload the images and storage the matrix in arrays for numeric treatment.\n",
    "* On the OpenCV library of Python, change the images to gray-scale and resize the photos to 180x200.\n",
    "* Finally, the outcome is a new dataset with the proper images for testing and modeling face recognition. With the Eigenfaces model to apply the Principal Component Analysis (PCA) so represent the face images in a low dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ImageUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd # Needs the package Pandas to be installed. Check Anaconda Environments and Packages.\n",
    "from sklearn.decomposition import PCA # Needs SciKit Learn package to be installed. Check Anaconda Environments and Packages.\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET FACES 94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face94_male = readFaces94MaleFaces(gray=True)\n",
    "plt.imshow(face94_male[0], plt.cm.gray);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, height, width = face94_male.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_faces = np.ones(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=200, whiten=True).fit(face94_male.reshape(N, height*width))\n",
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 4\n",
    "cols = 6\n",
    "plt.figure(figsize=(15,10))\n",
    "for i in np.arange(rows * cols):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(pca.components_[i].reshape(height, width), plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_face = pca.mean_.reshape(height, width)\n",
    "mean_face2 = np.mean(face94_male.reshape(N, height*width), axis=0).reshape(height, width)\n",
    "fig = plt.figure(figsize=(8,10))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "plt.title(\"PCA mean\")\n",
    "ax1.imshow(mean_face, plt.cm.gray)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "plt.title(\"np mean\")\n",
    "ax2.imshow(mean_face2, plt.cm.gray)\n",
    "Dis=np.linalg.norm(mean_face - mean_face2, ord=2, keepdims=False)\n",
    "print(\"Distance \"+ str(Dis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Median face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_face = np.median(face94_male.reshape(N, height*width), axis=0).reshape(height, width)\n",
    "plt.imshow(median_face, cmap=plt.cm.gray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#face94_male_projected = pca.transform(face94_male.reshape(N, height*width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images of natual landscapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The landscape images were obtain of **ImageNet** database [ImageNet database](http://image-net.org/) , \n",
    "each one of the directions is [online](http://image-net.org/api/text/imagenet.synset.geturls?wnid=n13104059). We use cv2 package by read and resize images, then we create an Numpy array with a gray scale of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landscapes = np.array(readLandsCapeImage(gray=True)) # Read dataset\n",
    "plt.imshow(landscapes[45], plt.cm.gray); # show image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_landscapes = np.zeros(landscapes.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.vstack((face94_male, landscapes))\n",
    "plt.imshow(dataset[-1], plt.cm.gray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.concatenate((labels_faces, labels_landscapes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_N, height, width = dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_with_noise = np.mean(dataset.reshape(dataset_N, height*width), axis=0).reshape(height, width)\n",
    "plt.imshow(mean_with_noise, cmap=plt.cm.gray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_with_noise = np.median(dataset.reshape(dataset_N, height*width), axis=0).reshape(height, width)\n",
    "plt.imshow(median_with_noise, cmap=plt.cm.gray);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show atypical data distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distance_info = getNormsAndDistanceInfoFromBaseImage(base_image=mean_with_noise, array_images=dataset, labels=labels)\n",
    "visualizeOutlierInfo(distance_info,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distance_info['falsitude_metrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show atypical data distances (outliers interquartile range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeOutlierInfo2(distance_info,dataset,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distance_info['falsitude_metrics_iqr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normas: Norm1.0  Norm2.0  Norm3.0  Norminf  Norm2.5  Norm0.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_norm = \"Norminf\"\n",
    "cols = 6\n",
    "rows = int(np.ceil(distance_info[\"outliers\"][selected_norm][\"indices\"].shape[0]/cols))\n",
    "plt.figure(figsize=(180,200))\n",
    "for i in np.arange(distance_info[\"outliers\"][selected_norm][\"indices\"].shape[0]):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(dataset[distance_info[\"outliers\"][selected_norm][\"indices\"][i]], plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_norm = \"Norm1.0\"\n",
    "selected_outliers = \"outliersiqr\"\n",
    "Distance=distance_info[\"norms\"][selected_norm][distance_info[selected_outliers][selected_norm]['indices']]\n",
    "Ind=distance_info[selected_outliers][selected_norm]['indices']\n",
    "Distance, Ind =zip(*sorted(zip(Distance, Ind)))\n",
    "cols = 6\n",
    "rows = int(np.ceil(len(Ind)/cols))\n",
    "plt.figure(figsize=(180,200))\n",
    "for i in np.arange(len(Ind)):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(dataset[Ind[-(i+1)]], plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_norm = dataset/255\n",
    "dataset_norm_cov = np.cov(dataset_norm.reshape(dataset_N, height*width))\n",
    "\n",
    "np.linalg.det(dataset_norm_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
